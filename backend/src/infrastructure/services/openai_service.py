"""
SERVI√áO OPENAI - VERS√ÉO INTELIGENTE
=====================================

Integra√ß√£o com a API da OpenAI.
Inclui:
- Mem√≥ria de contexto (retomar conversa)
- Varia√ß√£o de respostas (menos rob√≥tico)
- Detec√ß√£o de sentimento
- Sugest√µes proativas
"""

import json
import random
from datetime import datetime, timedelta
from openai import AsyncOpenAI
from src.config import get_settings

settings = get_settings()

# Cliente OpenAI (singleton)
client = AsyncOpenAI(api_key=settings.openai_api_key)


# ============================================
# VARIA√á√ïES DE SAUDA√á√ïES E RESPOSTAS
# ============================================

GREETING_VARIATIONS = {
    "formal": [
        "Ol√°! Como posso ajud√°-lo hoje?",
        "Ol√°! Em que posso ser √∫til?",
        "Bom dia! Como posso auxili√°-lo?",
        "Ol√°! Estou √† disposi√ß√£o para ajudar.",
    ],
    "cordial": [
        "Oi! üòä Como posso te ajudar?",
        "Ol√°! Tudo bem? Em que posso ajudar?",
        "Oi! Que bom falar com voc√™! Como posso ajudar?",
        "Ol√°! üëã Estou aqui pra te ajudar!",
    ],
    "informal": [
        "E a√≠! üëã Como posso te ajudar?",
        "Oi! Tudo certo? Bora l√°, como posso ajudar?",
        "Fala! üòÑ O que voc√™ precisa?",
        "Oi oi! Como posso te ajudar hoje?",
    ],
}

ACKNOWLEDGMENT_VARIATIONS = [
    "Entendi!",
    "Perfeito!",
    "√ìtimo!",
    "Certo!",
    "Legal!",
    "Beleza!",
    "Show!",
    "Anotado!",
]

TRANSITION_PHRASES = [
    "E me conta,",
    "Aproveitando,",
    "S√≥ pra eu entender melhor,",
    "E outra coisa,",
    "Ah, e",
    "Deixa eu perguntar,",
]


def get_random_greeting(tone: str = "cordial") -> str:
    """Retorna uma sauda√ß√£o aleat√≥ria baseada no tom."""
    greetings = GREETING_VARIATIONS.get(tone, GREETING_VARIATIONS["cordial"])
    return random.choice(greetings)


def get_random_acknowledgment() -> str:
    """Retorna uma frase de reconhecimento aleat√≥ria."""
    return random.choice(ACKNOWLEDGMENT_VARIATIONS)


def get_random_transition() -> str:
    """Retorna uma frase de transi√ß√£o aleat√≥ria."""
    return random.choice(TRANSITION_PHRASES)


# ============================================
# FUN√á√ïES PRINCIPAIS
# ============================================

async def chat_completion(
    messages: list[dict],
    model: str = None,
    temperature: float = 0.7,
    max_tokens: int = 500,
) -> dict:
    """
    Envia mensagens para OpenAI e retorna resposta.
    """
    response = await client.chat.completions.create(
        model=model or settings.openai_model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
    )

    return {
        "content": response.choices[0].message.content,
        "tokens_used": response.usage.total_tokens if response.usage else 0
    }


async def detect_sentiment(message: str) -> dict:
    """
    Detecta o sentimento/humor do lead na mensagem.
    
    Returns:
        {
            "sentiment": "positive" | "neutral" | "negative" | "frustrated" | "excited",
            "confidence": 0.0-1.0,
            "tone_adjustment": "sugest√£o de ajuste no tom"
        }
    """
    
    # An√°lise r√°pida por palavras-chave (sem chamar API para economizar)
    message_lower = message.lower()
    
    # Sinais de frustra√ß√£o/irrita√ß√£o
    frustrated_signals = [
        "demora", "lento", "ningu√©m responde", "p√©ssimo", "horr√≠vel",
        "n√£o funciona", "problema", "reclama√ß√£o", "absurdo", "vergonha",
        "desisto", "cancelar", "nunca mais", "pior", "raiva", "irritado",
        "cansado de", "toda vez", "sempre isso", "!!", "???",
    ]
    
    # Sinais de pressa/urg√™ncia
    urgent_signals = [
        "urgente", "r√°pido", "agora", "hoje", "preciso logo",
        "n√£o posso esperar", "emerg√™ncia", "prazo", "deadline",
    ]
    
    # Sinais positivos/anima√ß√£o
    positive_signals = [
        "obrigado", "obrigada", "perfeito", "√≥timo", "excelente",
        "maravilha", "adorei", "amei", "show", "top", "üòä", "üòÑ",
        "üëç", "üôè", "‚ù§Ô∏è", "muito bom", "gostei",
    ]
    
    # Sinais de interesse forte
    excited_signals = [
        "quero", "preciso", "vamos fechar", "quando come√ßa",
        "como fa√ßo", "me inscreve", "reserva", "!!",
    ]
    
    # Pontua cada categoria
    frustrated_score = sum(1 for s in frustrated_signals if s in message_lower)
    urgent_score = sum(1 for s in urgent_signals if s in message_lower)
    positive_score = sum(1 for s in positive_signals if s in message_lower)
    excited_score = sum(1 for s in excited_signals if s in message_lower)
    
    # Determina sentimento predominante
    if frustrated_score >= 2:
        return {
            "sentiment": "frustrated",
            "confidence": min(0.9, 0.5 + frustrated_score * 0.1),
            "tone_adjustment": "Seja emp√°tico, pe√ßa desculpas se necess√°rio, resolva o problema rapidamente. Evite frases gen√©ricas.",
            "detected_signals": "frustra√ß√£o/irrita√ß√£o"
        }
    elif urgent_score >= 1:
        return {
            "sentiment": "urgent",
            "confidence": min(0.9, 0.6 + urgent_score * 0.1),
            "tone_adjustment": "Seja direto e objetivo. Priorize resolver a urg√™ncia. Evite perguntas desnecess√°rias.",
            "detected_signals": "urg√™ncia/pressa"
        }
    elif excited_score >= 1 and positive_score >= 1:
        return {
            "sentiment": "excited",
            "confidence": 0.8,
            "tone_adjustment": "Mantenha a energia! Facilite o fechamento. Seja entusiasmado tamb√©m.",
            "detected_signals": "anima√ß√£o/interesse forte"
        }
    elif positive_score >= 1:
        return {
            "sentiment": "positive",
            "confidence": min(0.9, 0.6 + positive_score * 0.1),
            "tone_adjustment": "Continue positivo. Bom momento para avan√ßar na qualifica√ß√£o.",
            "detected_signals": "satisfa√ß√£o/positividade"
        }
    else:
        return {
            "sentiment": "neutral",
            "confidence": 0.5,
            "tone_adjustment": None,
            "detected_signals": None
        }


async def generate_context_aware_response(
    messages: list[dict],
    lead_data: dict,
    sentiment: dict,
    tone: str = "cordial",
    is_returning_lead: bool = False,
    hours_since_last_message: float = 0,
    previous_summary: str = None,
) -> dict:
    """
    Gera resposta consciente do contexto, sentimento e hist√≥rico.
    
    Args:
        messages: Hist√≥rico de mensagens
        lead_data: Dados extra√≠dos do lead
        sentiment: Resultado da detec√ß√£o de sentimento
        tone: Tom de voz configurado
        is_returning_lead: Se o lead est√° retornando ap√≥s um tempo
        hours_since_last_message: Horas desde √∫ltima mensagem
        previous_summary: Resumo da conversa anterior (se lead retornando)
    
    Returns:
        {
            "content": "resposta da IA",
            "tokens_used": int,
            "context_used": "descri√ß√£o do contexto aplicado"
        }
    """
    
    context_instructions = []
    
    # 1. Ajuste por sentimento
    if sentiment.get("tone_adjustment"):
        context_instructions.append(f"AJUSTE DE TOM: {sentiment['tone_adjustment']}")
    
    # 2. Lead retornando ap√≥s tempo
    if is_returning_lead and hours_since_last_message > 24:
        days = int(hours_since_last_message / 24)
        
        if previous_summary:
            context_instructions.append(f"""
LEAD RETORNANDO AP√ìS {days} DIA(S):
- Cumprimente de forma acolhedora
- Mencione brevemente o que conversaram antes: "{previous_summary}"
- Pergunte se ainda tem interesse ou se a situa√ß√£o mudou
- N√ÉO repita perguntas que j√° foram respondidas
- Exemplo: "Oi [nome]! Que bom te ver de volta! Da √∫ltima vez voc√™ estava interessado em [X]. Ainda est√° procurando?"
""")
        else:
            context_instructions.append(f"""
LEAD RETORNANDO AP√ìS {days} DIA(S):
- Cumprimente de forma acolhedora
- Mencione que j√° conversaram antes
- Pergunte como pode ajudar
- Exemplo: "Oi! Tudo bem? A gente j√° conversou h√° alguns dias. Como posso te ajudar agora?"
""")
    
    # 3. Contexto do lead para personaliza√ß√£o
    if lead_data:
        personalization = []
        
        if lead_data.get("name"):
            personalization.append(f"Use o nome '{lead_data['name']}' ocasionalmente (n√£o toda mensagem)")
        
        if lead_data.get("family_situation"):
            personalization.append(f"Situa√ß√£o familiar: {lead_data['family_situation']} - adapte sugest√µes")
        
        if lead_data.get("work_info"):
            personalization.append(f"Trabalho: {lead_data['work_info']} - considere na abordagem")
        
        if lead_data.get("budget_range"):
            personalization.append(f"Or√ßamento: {lead_data['budget_range']} - respeite a faixa")
        
        if lead_data.get("urgency_level"):
            personalization.append(f"Urg√™ncia: {lead_data['urgency_level']} - adapte o ritmo")
        
        if lead_data.get("preferences"):
            prefs = lead_data['preferences']
            if isinstance(prefs, list):
                prefs = ", ".join(prefs)
            personalization.append(f"Prefer√™ncias: {prefs} - use nas sugest√µes")
        
        if lead_data.get("pain_points"):
            pains = lead_data['pain_points']
            if isinstance(pains, list):
                pains = ", ".join(pains)
            personalization.append(f"Dores/Problemas: {pains} - mostre empatia e solu√ß√µes")
        
        if lead_data.get("objections"):
            objs = lead_data['objections']
            if isinstance(objs, list):
                objs = ", ".join(objs)
            personalization.append(f"‚ö†Ô∏è OBJE√á√ïES ANTERIORES: {objs} - contorne com argumentos")
        
        if lead_data.get("buying_signals"):
            signals = lead_data['buying_signals']
            if isinstance(signals, list):
                signals = ", ".join(signals)
            personalization.append(f"üî• SINAIS DE COMPRA: {signals} - ACELERE O FECHAMENTO!")
        
        if personalization:
            context_instructions.append("PERSONALIZA√á√ÉO (use naturalmente):\n" + "\n".join(f"- {p}" for p in personalization))
    
    # 4. Instru√ß√µes de varia√ß√£o
    context_instructions.append(f"""
VARIA√á√ÉO DE LINGUAGEM:
- N√ÉO repita as mesmas frases de mensagens anteriores
- Varie as sauda√ß√µes e transi√ß√µes
- Use linguagem natural, n√£o rob√≥tica
- Tom configurado: {tone}
- Evite come√ßar todas as mensagens com "Ol√°" ou "Oi"
""")
    
    # Monta mensagem de contexto
    if context_instructions:
        context_message = {
            "role": "system",
            "content": "INSTRU√á√ïES DE CONTEXTO PARA ESTA RESPOSTA:\n\n" + "\n\n".join(context_instructions)
        }
        # Insere ap√≥s o system prompt principal
        messages_with_context = [messages[0], context_message] + messages[1:]
    else:
        messages_with_context = messages
    
    # Chama a API
    response = await client.chat.completions.create(
        model=settings.openai_model,
        messages=messages_with_context,
        temperature=0.75,  # Um pouco mais criativo
        max_tokens=500,
    )
    
    return {
        "content": response.choices[0].message.content,
        "tokens_used": response.usage.total_tokens if response.usage else 0,
        "context_used": ", ".join([
            sentiment.get("detected_signals") or "neutral",
            "returning_lead" if is_returning_lead else "new_conversation",
            f"personalized:{len(lead_data or {})}_fields"
        ])
    }


async def generate_conversation_summary(conversation: list[dict]) -> str:
    """
    Gera um resumo curto da conversa para uso em retorno do lead.
    
    Returns:
        Resumo em 1-2 frases do que foi discutido
    """
    
    if len(conversation) < 2:
        return None
    
    summary_prompt = f"""Resuma esta conversa em 1-2 frases curtas, focando no que o cliente estava buscando.

CONVERSA:
{json.dumps(conversation[-10:], ensure_ascii=False, indent=2)}

Formato: Uma frase direta como "Voc√™ estava interessado em [X] e perguntou sobre [Y]"
N√ÉO inclua sauda√ß√µes ou formalidades.
M√°ximo 100 caracteres.

RESUMO:"""

    response = await client.chat.completions.create(
        model=settings.openai_model,
        messages=[{"role": "user", "content": summary_prompt}],
        temperature=0.3,
        max_tokens=100,
    )
    
    return response.choices[0].message.content.strip()


async def extract_lead_data(
    conversation: list[dict],
    required_fields: list[str],
    optional_fields: list[str],
) -> dict:
    """
    Extrai dados estruturados do lead a partir da conversa.
    Vers√£o expandida com contexto completo.
    """

    all_fields = required_fields + optional_fields

    extraction_prompt = f"""Analise a conversa abaixo e extraia TODAS as informa√ß√µes do cliente.

CAMPOS B√ÅSICOS A EXTRAIR:
{json.dumps(all_fields, ensure_ascii=False)}

CONTEXTO ADICIONAL A EXTRAIR (se mencionado):
- family_situation: Situa√ß√£o familiar (solteiro, casado, tem filhos, quantos filhos, etc)
- work_info: Informa√ß√µes sobre trabalho (onde trabalha, profiss√£o, regi√£o do trabalho)
- budget_range: Faixa de or√ßamento ou capacidade de pagamento
- urgency_level: N√≠vel de urg√™ncia (imediato, essa semana, esse m√™s, pesquisando, sem pressa)
- preferences: Lista de prefer√™ncias espec√≠ficas mencionadas
- pain_points: Problemas ou dores que o cliente mencionou
- objections: Obje√ß√µes ou preocupa√ß√µes levantadas (pre√ßo alto, precisa pensar, etc)
- decision_factors: O que √© importante para a decis√£o (localiza√ß√£o, pre√ßo, qualidade, etc)
- timeline: Prazo ou data mencionada
- previous_experience: Experi√™ncia anterior com produto/servi√ßo similar
- competitor_mentions: Se mencionou concorrentes ou alternativas
- buying_signals: Sinais de compra detectados (perguntou forma de pagamento, disponibilidade, etc)
- communication_style: Estilo de comunica√ß√£o do cliente (direto, detalhista, informal, formal)

REGRAS:
- Se a informa√ß√£o n√£o foi mencionada, use null
- Para listas (preferences, pain_points, etc), use array de strings
- Para buying_signals, liste frases que indicam inten√ß√£o de compra
- Seja espec√≠fico e extraia o m√°ximo de contexto poss√≠vel
- Retorne APENAS o JSON, sem explica√ß√µes

CONVERSA:
{json.dumps(conversation, ensure_ascii=False, indent=2)}

JSON:"""

    response = await client.chat.completions.create(
        model=settings.openai_model,
        messages=[{"role": "user", "content": extraction_prompt}],
        temperature=0.1,
        max_tokens=800,
    )

    try:
        content = response.choices[0].message.content.strip()
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        return json.loads(content)
    except (json.JSONDecodeError, IndexError):
        return {}


async def qualify_lead(
    conversation: list[dict],
    extracted_data: dict,
    qualification_rules: dict,
) -> dict:
    """
    Qualifica o lead como hot/warm/cold baseado na conversa.
    Vers√£o melhorada com an√°lise de sinais de compra.
    """

    qualification_prompt = f"""Analise a conversa e dados do lead para qualific√°-lo.

REGRAS DE QUALIFICA√á√ÉO:
- HOT (pronto para fechar): {', '.join(qualification_rules.get('hot', []))}
- WARM (interessado mas n√£o urgente): {', '.join(qualification_rules.get('warm', []))}
- COLD (apenas pesquisando): {', '.join(qualification_rules.get('cold', []))}

SINAIS DE COMPRA FORTES (indicam lead HOT):
- Pergunta sobre formas de pagamento ou parcelamento
- Pergunta sobre disponibilidade imediata
- Menciona prazo espec√≠fico ("preciso para", "at√© dia X")
- J√° pesquisou concorrentes e est√° comparando
- Pergunta sobre pr√≥ximos passos para fechar
- Demonstra urg√™ncia ou necessidade clara
- J√° tem or√ßamento definido

SINAIS DE OBJE√á√ÉO (podem indicar lead WARM ou COLD):
- "Vou pensar" - pode ser warm se outros sinais positivos
- "T√° caro" - warm se continua interessado
- "Depois eu vejo" - provavelmente cold
- "S√≥ pesquisando" - cold

DADOS COLETADOS:
{json.dumps(extracted_data, ensure_ascii=False, indent=2)}

CONVERSA:
{json.dumps(conversation, ensure_ascii=False, indent=2)}

Responda APENAS em JSON:
{{
  "qualification": "hot|warm|cold",
  "confidence": 0.0-1.0,
  "reason": "motivo da qualifica√ß√£o",
  "buying_signals_found": ["lista de sinais de compra detectados"],
  "objections_found": ["lista de obje√ß√µes detectadas"],
  "recommended_action": "a√ß√£o recomendada para o vendedor",
  "next_best_question": "pr√≥xima pergunta que a IA deveria fazer para avan√ßar"
}}

JSON:"""

    response = await client.chat.completions.create(
        model=settings.openai_model,
        messages=[{"role": "user", "content": qualification_prompt}],
        temperature=0.2,
        max_tokens=400,
    )

    try:
        content = response.choices[0].message.content.strip()
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        return json.loads(content)
    except (json.JSONDecodeError, IndexError):
        return {"qualification": "cold", "confidence": 0.5, "reason": "N√£o foi poss√≠vel qualificar"}


async def generate_lead_summary(
    conversation: list[dict],
    extracted_data: dict,
    qualification: dict,
) -> str:
    """
    Gera um resumo estruturado do lead para o gestor.
    Vers√£o melhorada com contexto completo.
    """

    summary_prompt = f"""Gere um resumo COMPLETO e ESTRUTURADO deste lead para a equipe comercial.

DADOS DO LEAD:
{json.dumps(extracted_data, ensure_ascii=False, indent=2)}

QUALIFICA√á√ÉO: {qualification.get('qualification', 'N/A')} ({qualification.get('confidence', 0)*100:.0f}% confian√ßa)
MOTIVO: {qualification.get('reason', '')}
SINAIS DE COMPRA: {', '.join(qualification.get('buying_signals_found', []))}
OBJE√á√ïES: {', '.join(qualification.get('objections_found', []))}
A√á√ÉO RECOMENDADA: {qualification.get('recommended_action', '')}

CONVERSA:
{json.dumps(conversation[-8:], ensure_ascii=False, indent=2)}

Formato do resumo:
üìã RESUMO DO LEAD
- O que busca:
- Situa√ß√£o:
- Urg√™ncia:
- Or√ßamento:
- Obje√ß√µes a contornar:
- Pontos a destacar na abordagem:
- Pr√≥ximo passo recomendado:

Seja direto e √∫til para o vendedor fechar a venda.

RESUMO:"""

    response = await client.chat.completions.create(
        model=settings.openai_model,
        messages=[{"role": "user", "content": summary_prompt}],
        temperature=0.3,
        max_tokens=400,
    )

    return response.choices[0].message.content.strip()


async def generate_proactive_suggestions(
    conversation: list[dict],
    extracted_data: dict,
    niche: str,
) -> dict:
    """
    Gera sugest√µes proativas baseadas no contexto do lead.

    Returns:
        {
            "suggestions": ["sugest√£o 1", "sugest√£o 2"],
            "urgency_message": "mensagem de urg√™ncia se aplic√°vel",
            "next_step": "pr√≥ximo passo recomendado",
            "personalized_pitch": "argumento personalizado"
        }
    """

    suggestion_prompt = f"""Com base no contexto do lead, gere sugest√µes proativas para a IA usar na conversa.

NICHO: {niche}

DADOS DO LEAD:
{json.dumps(extracted_data, ensure_ascii=False, indent=2)}

CONVERSA RECENTE:
{json.dumps(conversation[-6:], ensure_ascii=False, indent=2)}

Gere em JSON:
{{
    "suggestions": ["at√© 3 sugest√µes espec√≠ficas baseadas no perfil do lead"],
    "urgency_message": "mensagem de urg√™ncia se o lead demonstrou interesse (ou null)",
    "next_step": "pr√≥ximo passo natural da conversa",
    "personalized_pitch": "argumento de venda personalizado para este lead espec√≠fico",
    "objection_responses": {{"obje√ß√£o": "resposta para contornar"}}
}}

REGRAS:
- Sugest√µes devem ser espec√≠ficas para o perfil (ex: se tem filhos, sugira op√ß√µes family-friendly)
- Urgency s√≥ se houver motivo real (disponibilidade limitada, promo√ß√£o, etc)
- Personalized pitch deve usar informa√ß√µes do lead (trabalho, fam√≠lia, prefer√™ncias)
- Seja natural, n√£o agressivo

JSON:"""

    response = await client.chat.completions.create(
        model=settings.openai_model,
        messages=[{"role": "user", "content": suggestion_prompt}],
        temperature=0.4,
        max_tokens=500,
    )

    try:
        content = response.choices[0].message.content.strip()
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        return json.loads(content)
    except (json.JSONDecodeError, IndexError):
        return {
            "suggestions": [],
            "urgency_message": None,
            "next_step": "Continuar qualifica√ß√£o",
            "personalized_pitch": None,
            "objection_responses": {}
        }


# ============================================
# HELPERS PARA DELAY HUMANIZADO
# ============================================

def calculate_typing_delay(message_length: int) -> float:
    """
    Calcula delay de digita√ß√£o baseado no tamanho da mensagem.
    
    Args:
        message_length: N√∫mero de caracteres da resposta
    
    Returns:
        Delay em segundos (entre 1 e 5)
    """
    # Simula ~40 palavras por minuto de digita√ß√£o
    # ~5 caracteres por palavra em m√©dia
    words = message_length / 5
    seconds = words / 40 * 60
    
    # Limita entre 1 e 5 segundos
    delay = max(1.0, min(5.0, seconds))
    
    # Adiciona varia√ß√£o aleat√≥ria de ¬±20%
    variation = delay * 0.2 * (random.random() * 2 - 1)
    
    return round(delay + variation, 1)