import logging
import json
import os
import asyncio
import numpy as np
from typing import List, Dict, Any, Optional
from src.infrastructure.llm.factory import LLMFactory

logger = logging.getLogger(__name__)

INDEX_PATH = os.path.join(os.getcwd(), "storage", "property_embeddings.json")

class SemanticSearchService:
    """
    Servi√ßo de Busca Sem√¢ntica usando Embeddings e Similaridade de Cosseno.
    Permite encontrar im√≥veis por inten√ß√£o/contexto.
    """
    
    def __init__(self):
        self.index: Dict[str, List[float]] = {}
        self.properties: Dict[str, Dict] = {}
        self._load_index()

    def _load_index(self):
        """Carrega o √≠ndice do disco."""
        try:
            if os.path.exists(INDEX_PATH):
                with open(INDEX_PATH, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.index = data.get("embeddings", {})
                    self.properties = data.get("properties", {})
                logger.info(f"üíæ √çndice sem√¢ntico carregado: {len(self.index)} itens")
            else:
                logger.info("üÜï Novo √≠ndice sem√¢ntico ser√° criado.")
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar √≠ndice: {e}")

    def _save_index(self):
        """Salva o √≠ndice no disco."""
        try:
            os.makedirs(os.path.dirname(INDEX_PATH), exist_ok=True)
            with open(INDEX_PATH, 'w', encoding='utf-8') as f:
                json.dump({
                    "embeddings": self.index,
                    "properties": self.properties
                }, f, ensure_ascii=False, indent=2)
            logger.info("‚úÖ √çndice sem√¢ntico salvo com sucesso.")
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar √≠ndice: {e}")

    async def index_properties(self, properties: List[Dict]):
        """
        Gera embeddings para uma lista de im√≥veis e atualiza o √≠ndice.
        """
        provider = LLMFactory.get_provider()
        updated = False
        
        for prop in properties:
            codigo = str(prop.get("codigo"))
            if not codigo: continue
            
            # Se j√° indexado, pula (poder√≠amos adicionar l√≥gica de hashes para mudar se a descri√ß√£o mudar)
            if codigo in self.index:
                continue
                
            # Prepara texto para embedding (T√≠tulo + Tipo + Bairro + Descri√ß√£o)
            text_to_embed = f"{prop.get('titulo', '')} {prop.get('tipo', '')} em {prop.get('regiao', '')}. {prop.get('descricao', '')}"
            text_to_embed = text_to_embed.strip()
            
            if not text_to_embed: continue
            
            try:
                logger.info(f"üß† Gerando embedding para im√≥vel {codigo}...")
                embedding = await provider.generate_embeddings(text_to_embed)
                self.index[codigo] = embedding
                self.properties[codigo] = prop
                updated = True
                
                # Pequeno delay para evitar rate limit massivo se for carga inicial grande
                # await asyncio.sleep(0.05) 
            except Exception as e:
                logger.error(f"‚ùå Falha ao indexar im√≥vel {codigo}: {e}")
        
        if updated:
            self._save_index()

    async def search(self, query: str, limit: int = 5, min_score: float = 0.7) -> List[Dict]:
        """
        Busca im√≥veis semanticamente similares √† query.
        """
        if not self.index:
            logger.warning("‚ö†Ô∏è Busca sem√¢ntica falhou: √≠ndice vazio.")
            return []
            
        try:
            provider = LLMFactory.get_provider()
            query_embedding = await provider.generate_embeddings(query)
            query_vec = np.array(query_embedding)
            
            results = []
            
            for codigo, embedding in self.index.items():
                prop_vec = np.array(embedding)
                
                # Similaridade de Cosseno = (A . B) / (||A|| * ||B||)
                norm_a = np.linalg.norm(query_vec)
                norm_b = np.linalg.norm(prop_vec)
                
                if norm_a == 0 or norm_b == 0:
                    score = 0
                else:
                    score = np.dot(query_vec, prop_vec) / (norm_a * norm_b)
                
                if score >= min_score:
                    prop_data = self.properties[codigo].copy()
                    prop_data["semantic_score"] = float(score)
                    results.append(prop_data)
            
            # Ordena por score descendente
            results.sort(key=lambda x: x["semantic_score"], reverse=True)
            
            logger.info(f"üîé Busca sem√¢ntica: '{query}' -> {len(results[:limit])} resultados")
            return results[:limit]
            
        except Exception as e:
            logger.error(f"‚ùå Erro na busca sem√¢ntica: {e}")
            return []

# Singleton para uso global
semantic_search = SemanticSearchService()
